{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install fasttext==0.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_data = '../data/phish/'\n",
    "legitimate_data = '../data/legit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.decode('utf-8')\n",
    "    while '\\n' in text:\n",
    "        text = text.replace('\\n', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    words = text.split()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    stripped = []\n",
    "    for token in words: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            stripped.append(new_token.lower())\n",
    "    text = ' '.join(stripped)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    text_list = list()\n",
    "    files = os.listdir(path)\n",
    "    for text_file in files:\n",
    "        file_path = os.path.join(path, text_file)\n",
    "        read_file = open(file_path,'r+')\n",
    "        read_text = read_file.read()\n",
    "        read_file.close()\n",
    "        cleaned_text = clean_text(read_text)\n",
    "        text_list.append(cleaned_text)\n",
    "    return text_list, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train_0, temp = get_data(phishing_data)\n",
    "no_head_train_1, temp = get_data(legitimate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train = no_head_train_0 + no_head_train_1\n",
    "no_head_labels_train = ([0] * len(no_head_train_0)) + ([1] * len(no_head_train_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "X = tf_vectorizer.fit_transform(no_head_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#total words', 166433)\n",
      "('#unique words', 23095)\n"
     ]
    }
   ],
   "source": [
    "print ('#total words', np.matrix.sum(X.todense()))\n",
    "print ('#unique words',len(set(tf_vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(len(no_head_labels_train))\n",
    "train_data = np.array(no_head_train)[shuffled_indices]\n",
    "train_data = train_data.tolist()\n",
    "train_label = np.array(no_head_labels_train)[shuffled_indices]\n",
    "train_label = train_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_data = train_data[0:int(0.8*len(train_data))]\n",
    "temp_train_label = train_label[0:int(0.8*len(train_label))]\n",
    "temp_test_data = train_data[int(0.8*len(train_data)):]\n",
    "temp_test_labels = train_label[int(0.8*len(train_label)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_train_file = '../data/fast_train.txt'\n",
    "fast_test_file = '../data/fast_test.txt'\n",
    "writeFile = open(fast_train_file, 'w')\n",
    "for text, label in zip(temp_train_data, temp_train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()\n",
    "\n",
    "writeFile = open(fast_test_file, 'w')\n",
    "for text, label in zip(temp_test_data, temp_test_labels):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "import numpy as np\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(fast_train_file, 'trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.test(fast_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9122137404580153, 0.9122137404580153, 262)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.precision, result.recall, result.nexamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 5, 1, 'utf-8', u'softmax', 0, 0.0001)\n"
     ]
    }
   ],
   "source": [
    "print (classifier.min_count, classifier.dim, classifier.epoch, classifier.word_ngrams, classifier.encoding, classifier.loss_name, classifier.maxn, classifier.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
