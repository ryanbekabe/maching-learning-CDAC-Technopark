{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "phishing_data = '../data/phish/'\n",
    "legitimate_data = '../data/legit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.decode('utf-8')\n",
    "    while '\\n' in text:\n",
    "        text = text.replace('\\n', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    words = text.split()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    stripped = []\n",
    "    for token in words: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            stripped.append(new_token.lower())\n",
    "    text = ' '.join(stripped)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    text_list = list()\n",
    "    files = os.listdir(path)\n",
    "    for text_file in files:\n",
    "        file_path = os.path.join(path, text_file)\n",
    "        read_file = open(file_path,'r+')\n",
    "        read_text = read_file.read()\n",
    "        read_file.close()\n",
    "        cleaned_text = clean_text(read_text)\n",
    "        text_list.append(cleaned_text)\n",
    "    return text_list, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train_0, temp = get_data(phishing_data)\n",
    "no_head_train_1, temp = get_data(legitimate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train = no_head_train_0 + no_head_train_1\n",
    "no_head_labels_train = ([0] * len(no_head_train_0)) + ([1] * len(no_head_train_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "X = tf_vectorizer.fit_transform(no_head_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#total words', 166433)\n",
      "('#unique words', 23095)\n"
     ]
    }
   ],
   "source": [
    "print ('#total words', np.matrix.sum(X.todense()))\n",
    "print ('#unique words',len(set(tf_vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabularymat(TEXTFILES,VOC,PLAY,METHOD):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    if (METHOD == \"TFIDF\"):\n",
    "        voc = TfidfVectorizer()\n",
    "        voc.fit(VOC)\n",
    "    \n",
    "        if (PLAY == \"TRAIN\"):\n",
    "            TrainMat = voc.transform(TEXTFILES)\n",
    "            return TrainMat\n",
    "\n",
    "        if (PLAY ==\"TEST\"):\n",
    "            TestMat = voc.transform(TEXTFILES)\n",
    "            return TestMat\n",
    "    \n",
    "    if (METHOD == \"TDM\"):\n",
    "        voc = CountVectorizer()\n",
    "        voc.fit(VOC)\n",
    "    \n",
    "        if (PLAY == \"TRAIN\"):\n",
    "            TrainMat = voc.transform(TEXTFILES)\n",
    "            return TrainMat\n",
    "\n",
    "        if (PLAY ==\"TEST\"):\n",
    "            TestMat = voc.transform(TEXTFILES)\n",
    "            return TestMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainMat = vocabularymat(no_head_train,no_head_train,PLAY= \"TRAIN\",METHOD=\"TDM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TrainMat.todense()\n",
    "datalabel = no_head_labels_train\n",
    "\n",
    "Traindata = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featurelearning(Data, Method):\n",
    "    from sklearn.decomposition import TruncatedSVD, NMF\n",
    "    if (Method == \"SVD\"):\n",
    "        model = TruncatedSVD(n_components=30, n_iter=7, random_state=42)\n",
    "        Matrix = model.fit_transform(Data)\n",
    "    if (Method == \"NMF\"):\n",
    "        model = NMF(n_components=30, init='random', random_state=0)\n",
    "        Matrix = model.fit_transform(Data)\n",
    "    return Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Featurelearning(Traindata, Method=\"NMF\")\n",
    "y_train = datalabel\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Accuracy\n",
      "0.838\n",
      "precision\n",
      "0.816\n",
      "recall\n",
      "0.882\n",
      "f-score\n",
      "0.848\n",
      "[[167  44]\n",
      " [ 26 195]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "expected = y_test\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted, average=\"binary\")\n",
    "precision = precision_score(expected, predicted , average=\"binary\")\n",
    "f1 = f1_score(expected, predicted , average=\"binary\")\n",
    "\n",
    "print(model)\n",
    "print(\"Accuracy\")\n",
    "print(\"%.3f\" %accuracy)\n",
    "print(\"precision\")\n",
    "print(\"%.3f\" %precision)\n",
    "print(\"recall\")\n",
    "print(\"%.3f\" %recall)\n",
    "print(\"f-score\")\n",
    "print(\"%.3f\" %f1)\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
